<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blink-Detector & WebGazer Dashboard</title>

    <!-- Calibration / layout styles -->
    <link rel="stylesheet" href="stylesheet.css">

    <!-- Minimal inline styles ‚Äì only for the detector block -->
    <style>
        body {
            font-family: system-ui, sans-serif;
        }

        /* ‚îÄ‚îÄ Blink-detector styles ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
        #wrapper {
            position: relative;
            margin: 1.5rem auto;
        }

        video {
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, .25);
        }

        #canvas {
            position: absolute;
            top: 0;
            left: 0;
        }

        #status {
            position: absolute;
            top: 8px;
            left: 8px;
            padding: 4px 8px;
            border-radius: 4px;
            background: rgba(0, 0, 0, .65);
            color: #fff;
            font-weight: 600;
            font-size: 1rem;
            white-space: nowrap;
            pointer-events: none;
        }
    </style>

    <!-- OpenCV full build -->
    <script defer src="https://docs.opencv.org/4.5.5/opencv.js"></script>

    <!-- Prepare OpenCV bootstrap -->
    <script>
        window.Module = {
            onRuntimeInitialized() {
                if (!cv.FS_createDataFile) {
                    const FS = cv.FS || self.FS || Module.FS;
                    if (FS?.createDataFile) cv.FS_createDataFile = (...a) => FS.createDataFile(...a);
                    else if (FS?.writeFile) cv.FS_createDataFile = (_r, n, d) => FS.writeFile('/' + n, d, { encoding: 'binary' });
                    else console.error('Error no FS helper found OpenCV build incompatible');
                }
                initApp();
            }
        };
    </script>
</head>

<body>
    <!-- ‚îÄ‚îÄ A. Blink / Eye-Closure detector ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <h1 style="text-align:center;margin-top:1rem;">Blink &amp; Eye-Closure Detector (<code>opencv.js</code>)</h1>
    <div id="wrapper">
        <video id="video" autoplay muted playsinline width="480" height="360"></video>
        <canvas id="canvas" width="480" height="360"></canvas>
        <div id="status">Loading ‚Ä¶</div>
    </div>

    <!-- ‚îÄ‚îÄ B. WebGazer / calibration grid ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <h2 style="text-align:center;">WebGazer Calibration Controls</h2>
    <div class="height50">
        <div id="left">LEFT</div>
        <div id="forward">FORWARD <button id="endcalibration">End Calibration</button></div>
        <div id="up">UP</div>
    </div>
    <div class="height50">
        <div id="right">RIGHT</div>
        <div id="backward">BACKWARD</div>
        <div id="down">DOWN</div>
    </div>

    <!-- External scripts for WebGazer/WebSocket -->
    <script src="https://webgazer.cs.brown.edu/webgazer.js"></script>
    <script src="collectdata.js"></script>
    <script>
        window.addEventListener('DOMContentLoaded', () => {
            /* start WebGazer */
            webgazer
                .setGazeListener((data, elapsedTime) => {
                    /* optional: do something with `data` */
                })
                .begin()                    // ‚¨ÖÔ∏è starts the tracker
                .showPredictionPoints(true) // ‚¨ÖÔ∏è draws the red dot(s)
                .showVideo(false);          // hide the mirror video (CSS already hides it)
        });
    </script>


    <!-- Detector logic (unchanged) -->
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const statusEl = document.getElementById('status');

        async function initCamera() {
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;
            return new Promise(r => video.onloadedmetadata = r);
        }

        const CASCADES = {
            face: { local: 'face.xml', remote: 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml' },
            eyes: { local: 'eyes.xml', remote: 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye_tree_eyeglasses.xml' }
        };
        async function loadClassifier(name, file) {
            const { local, remote } = CASCADES[name];
            let buf;
            try { buf = await (await fetch(local)).arrayBuffer(); }
            catch { buf = await (await fetch(remote)).arrayBuffer(); }
            cv.FS_createDataFile('/', file, new Uint8Array(buf), true, false, false);
            const cc = new cv.CascadeClassifier();
            if (!cc.load(file)) throw new Error(`${name} cascade failed`);
            return cc;
        }

        let faceCascade, eyeCascade, src, gray;
        const MISS = 6, seen = { L: MISS, R: MISS };

        function processFrame() {
            if (video.readyState < 2) { requestAnimationFrame(processFrame); return; }

            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            src.data.set(ctx.getImageData(0, 0, src.cols, src.rows).data);
            cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY);
            cv.equalizeHist(gray, gray);

            const faces = new cv.RectVector(), eyes = new cv.RectVector();
            faceCascade.detectMultiScale(gray, faces, 1.2, 3, 0, new cv.Size(80, 80));

            const hit = { L: false, R: false };
            for (let i = 0; i < faces.size(); ++i) {
                const f = faces.get(i), roi = gray.roi(f);
                eyeCascade.detectMultiScale(roi, eyes, 1.15, 3, 0, new cv.Size(20, 20));
                for (let j = 0; j < eyes.size(); ++j) {
                    const e = eyes.get(j), cx = f.x + e.x + e.width / 2;
                    const side = cx < f.x + f.width / 2 ? 'L' : 'R';
                    hit[side] = true;
                    cv.rectangle(src,
                        new cv.Point(f.x + e.x, f.y + e.y),
                        new cv.Point(f.x + e.x + e.width, f.y + e.y + e.height),
                        side === 'L' ? [0, 255, 0, 255] : [0, 0, 255, 255], 2);
                }
                roi.delete();
            }

            ['L', 'R'].forEach(s => { hit[s] ? seen[s] = MISS : seen[s] > 0 && seen[s]--; });

            const leftOpen = seen.L > 0, rightOpen = seen.R > 0;
            statusEl.textContent = leftOpen && rightOpen ? 'NONE CLOSED'
                : !leftOpen && !rightOpen ? 'BOTH CLOSED'
                    : !leftOpen ? 'LEFT CLOSED' : 'RIGHT CLOSED';

            cv.imshow('canvas', src);
            faces.delete(); eyes.delete();
            requestAnimationFrame(processFrame);
        }

        async function initApp() {
            try {
                statusEl.textContent = 'Starting camera‚Ä¶';
                await initCamera();

                statusEl.textContent = 'Loading classifiers‚Ä¶';
                [faceCascade, eyeCascade] = await Promise.all([
                    loadClassifier('face', 'face.xml'),
                    loadClassifier('eyes', 'eyes.xml')
                ]);

                src = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC4);
                gray = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC1);

                statusEl.textContent = 'Detecting eyes‚Ä¶';
                requestAnimationFrame(processFrame);
            } catch (err) {
                console.error(err);
                statusEl.textContent = 'üö´ ' + err.message;
            }
        }
    </script>
</body>

</html>